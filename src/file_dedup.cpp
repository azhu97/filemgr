// Go through files and identify duplicates based on content hash
// Move duplicates to a "DUPLICATES" folder within the Downloads directory
#include <iostream>
#include <filesystem>
#include <map>
#include <vector>
#include "file_ops.hpp" // for downloadPath()
#include <CommonCrypto/CommonDigest.h> // for CC_SHA256
#include <fstream>
#include <unordered_set>
#include <sys/stat.h>
#include <ctime>

namespace fs = std::filesystem;

std::unordered_set<std::string> filemgr_directories = {
    "IMAGES", "VIDEOS", "AUDIO", "DOCUMENTS", "ARCHIVES", "INSTALLERS"
};

std::string computeFileHash(const fs::path& file_path) {
    std::ifstream file(file_path, std::ifstream::binary);

    if (!file) {
        return "";
    }

    CC_SHA256_CTX ctx;
    CC_SHA256_Init(&ctx);

    const size_t buffer_size = 32768;
    char buffer[buffer_size];

    while (file.good()) {
        file.read(buffer, buffer_size);
        CC_SHA256_Update(&ctx, buffer, file.gcount());
    }

    unsigned char hash[CC_SHA256_DIGEST_LENGTH];
    CC_SHA256_Final(hash, &ctx);

    // Convert hash from bytes to hex string
    std::string hash_str;

    for (int i = 0; i < CC_SHA256_DIGEST_LENGTH; ++i) {
        char buf[3];
        snprintf(buf, sizeof(buf), "%02x", hash[i]);
        hash_str += buf;
    }

    return hash_str;
}

void deduplicateFiles() {
    std::string download_path = downloadPath();
    std::string duplicates_path = download_path + "/DUPLICATES";
    std::cout << "Deduplicating files in: " << download_path << "\n";
    std::unordered_map<std::string, fs::path> hash_map; // formatted as computed_file_hash -> file_path

    fs::create_directory(duplicates_path);

    // First pass: compute hashes and group files
    // Make sure to look inside directories created by the sorter
    for (const auto& entry : fs::recursive_directory_iterator(download_path)) {
        if (!entry.is_regular_file() && (entry.is_directory() && filemgr_directories.find(entry.path().filename().string()) == filemgr_directories.end())) {
            continue; // skip non-regular files and directories not generated by filemngr directories
        }

        // compute file hash 
        std::string file_hash = computeFileHash(entry.path());

        if (hash_map.count(file_hash) == 0) {
            hash_map[file_hash] = entry.path(); // no need to move anything to the DUPLICATES folder
        } else {
            // check which should be moved to the DUPLICATES folder 
            fs::path old_file = hash_map[file_hash];
            fs::path new_file = entry.path();

            struct stat old_stat;
            struct stat new_stat;
            time_t old_time;
            time_t new_time;

            stat(old_file.c_str(), &old_stat);
            stat(new_file.c_str(), &new_stat);
            old_time = old_stat.st_birthtimespec.tv_sec;
            new_time = new_stat.st_birthtimespec.tv_sec;

            if (old_time > new_time) {
                // the previous file is older, so move the new file to the DUPLICATES folder
                rename(new_file, duplicates_path + "/" + new_file.filename().string());
            } else { 
                rename(old_file, duplicates_path + "/" + old_file.filename().string());
                hash_map[file_hash] = new_file;
            }
        }
    }
}

int main() {
    deduplicateFiles();
    return 0;
}